# visual-question-answering

## Visual Question Answering with Easy-VQA

This repository explores Visual Question Answering (VQA) using the Easy-VQA dataset, a simplified dataset designed to facilitate quick experimentation and learning. VQA combines computer vision and natural language processing to answer questions about images, making it a compelling task for developing and testing multimodal AI models. Easy-VQA offers a straightforward entry point for beginners, featuring a limited set of questions and corresponding images, enabling efficient model training and evaluation. This project includes code for data preprocessing, model training, and evaluation, along with examples to help users get started with their VQA experiments.

## DataSet
The Dataset can be easily downloaded/installed as shown below:

!pip install -qqq easy-vqa

<img width="1016" alt="Screenshot 2024-07-17 at 11 53 05â€¯AM" src="https://github.com/user-attachments/assets/df1e8697-8194-4a0f-8c34-8e4617bdcfcf">


The output of this Dataset is categorical as we have limited number of one word output labels. Of course when we will try with a complex dataset we will see long text answers.

## Model High-Level Architecture


![easyvqa2](https://github.com/user-attachments/assets/4d1057c0-d36b-48b8-9eda-a3baa4ea47d9)
